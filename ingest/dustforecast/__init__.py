import logging
import os
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import requests
import xarray as xr
import xmltodict

from ingest import DataIngest, ParameterMissing
from ingest.utils import download_file_temp, convert_data


class DustForecastIngest(DataIngest):
    def __init__(self, dataset_id, output_dir, username, password, timeout=None):
        super().__init__(dataset_id=dataset_id, output_dir=output_dir)

        if not username:
            raise ParameterMissing("username required")

        if not password:
            raise ParameterMissing("password required")

        self.username = username
        self.password = password

        self.timeout = timeout

        self.variables = [
            {
                "variable": "OD550_DUST",
                "name": "od550_dust",
                "desc": "Multi-Model Dust Optical Depth (550nm)",
            },
            {
                "variable": "SCONC_DUST",
                "name": "sconc_dust",
                "desc": "Multi-Model Dust Surface Concentration (µg/m³)",
                "units": "ppb",
                "convert": {
                    "operation": "multiply",
                    "constant": 1000000000,
                    "units": "μgm**3"
                }
            }
        ]

    def run(self):
        logging.info('[DUST_FORECAST]: Starting Process...')
        dataset_state = self.get_state()
        url = "https://dust.aemet.es/thredds/catalog/restrictedDataRoot/MULTI-MODEL/latest/catalog.xml"

        logging.info(f'[DUST_FORECAST]: Getting  catalog with url: {url}')
        r = requests.get(url, timeout=self.timeout)

        logging.debug(f'[DUST_FORECAST]: Parsing  catalog')
        data = xmltodict.parse(r.text)

        dataset_name = data["catalog"]["dataset"]["dataset"]["@name"]

        data_file_date = dataset_name.split("_")[0]

        data_file_date = datetime.strptime(data_file_date, "%Y%m%d").isoformat()

        logging.info(f"[DUST_FORECAST]: Latest date from remote catalog is: {data_file_date}")

        if dataset_state and dataset_state.get("last_update") and dataset_state.get(
                "last_update") == data_file_date:
            logging.info(f'[DUST_FORECAST]: No Update required. Skipping...')
            return
        else:
            file_url = f"https://dust.aemet.es/thredds/fileServer/restrictedDataRoot/MULTI-MODEL/latest/{dataset_name}"
            logging.info(f"[DUST_FORECAST]: Downloading data from url: {file_url}' for date: {data_file_date}")
            tmp_file = download_file_temp(file_url, auth=(self.username, self.password), timeout=self.timeout)

            logging.debug(f"[DUST_FORECAST]: Forecast downloaded successfully to temp file: {tmp_file}")

            processed = self.process(tmp_file)

            if processed:
                for variable in self.variables:
                    param = variable.get("name")

                    data_dir = f"{self.output_dir}/{param}"

                    # cleanup old data
                    self.cleanup_old_data(data_file_date, data_dir)

                    # Send ingest command
                    ingest_payload = {
                        "namespace": f"-n {param}",
                        "path": f"-p {data_dir}",
                        "datatype": "-t tif",
                        "args": "-x -conf /rulesets/namespace_yyy-mm-ddTH.tif.json"
                    }

                    logging.info(
                        f"[DUST_FORECAST]: Sending ingest command for param: {param} and date {data_file_date}")
                    self.send_ingest_command(ingest_payload)

                self.update_state(data_file_date)

    def process(self, temp_file):
        logging.info(f"[DUST_FORECAST]: Processing data...")

        ds = xr.open_dataset(temp_file)

        ds.rio.write_crs("epsg:4326", inplace=True)

        # ds = self.clip_to_africa(ds)

        for var in self.variables:
            variable = var.get("variable")
            param = var.get("name")

            logging.debug(f"[DUST_FORECAST]: Processing variable: {variable}")
            if variable in ds.variables:
                for i, t in enumerate(ds.time.values):
                    logging.debug(f"[DUST_FORECAST]: Processing date: {t}")
                    data_datetime = pd.to_datetime(str(t))
                    date_str = data_datetime.strftime("%Y-%m-%dT%H:%M:%S.000Z")
                    param_t_filename = f"{self.output_dir}/{param}/{param}_{date_str}.tif"

                    # create directory if not exists
                    Path(param_t_filename).parent.absolute().mkdir(parents=True, exist_ok=True)

                    data_array = ds[variable].isel(time=i)
                    nodata_value = data_array.encoding.get('nodata', data_array.encoding.get('_FillValue'))

                    if var.get("convert"):
                        convert_config = var.get("convert")
                        operation = convert_config.get("operation")
                        constant = convert_config.get("constant")
                        units = convert_config.get("units")

                        if operation and constant:
                            logging.info(f"[DUSt_FORECAST]: Performing operation : {operation} on data")
                            data_array = convert_data(data_array, constant, operation)

                            if units:
                                data_array.attrs["units"] = units

                    # check that nodata is not nan
                    if np.isnan(nodata_value):
                        data_array = data_array.rio.write_nodata(-9999, encoded=True)

                    data_array.rio.to_raster(param_t_filename, driver="COG")

        # remove downloaded file
        os.remove(temp_file)

        return True
